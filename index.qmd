---
title: "Predicting the President: An exploration and comparison of classification algorithms."

bibliography: references.bib
---

# 1. Introduction.

The SONA (State of the Nation Address) is a highlight of the South African political calendar. Every year, and after every election, the South African president addresses parliament and the nation with a speech that highlights what the government has achieved over the past year and outlines the government's important objectives over the next year. Since the beginning of 1994, there have been 36 SONA's, by 6 presidents, each with their own distinct tone and diction. The distinct nature of these speeches and their structure suggest that given a sentence from one of the speeches, the president who delivered it is identifiable to a human who has seen the speeches before. Could predictive classification models identify the president solely from a sentence from one of their speeches?

Analysis of the speeches given by presidents of other countries is abundant in the literature [*see @miranda2021exploring; @najarzadegan2017critical; @budiharto2018prediction*]. These analyses focus on sentiment analysis and topic modelling. The purpose of this paper is to compare the ability of three classification algorithms to predict the South African president given a sentence from one of their speeches. The three classification algorithms that are assessed in this paper are: Classification Trees, Random Forests and Feed Forward Neural Networks.

# 2. Methodology.

## Data

Each of the 36 SONA's undergo cleaning and manipulation to create datasets that are usable by the classification models in the prediction process.

### Data Cleaning

One data frame is created using all 36 speeches. The dataframe consists of the speech identifier (in this instance, the name of the text file for that speech), and the speech itself. The name of the president who gave the speech and the year the speech was delivered are extracted from the speech identifier. The speech date is added to the dataframe by extracting it from the speech itself. Additionally, all unnecessary text is removed from each of the speeches.

```{r echo=FALSE}

### Libraries (that need to be ran when knitting the file)

suppressMessages(library(tidyverse))
library(tidytext)
library(tokenizers)
library(gghighlight)
library(tictoc)

```

```{r eval=FALSE, echo=FALSE}

### Libraries (that do not need to be ran when knitting the file)

library(rpart)
library(ranger)
library(keras)
library(reticulate)
library(tensorflow)

## Cleaning Speech Data into usable dataframes

filenames = c('1994_post_elections_Mandela.txt', '1994_pre_elections_deKlerk.txt', '1995_Mandela.txt', '1996_Mandela.txt', '1997_Mandela.txt', '1998_Mandela.txt', 
               '1999_post_elections_Mandela.txt', '1999_pre_elections_Mandela.txt', '2000_Mbeki.txt', '2001_Mbeki.txt', '2002_Mbeki.txt', '2003_Mbeki.txt', 
               '2004_post_elections_Mbeki.txt', '2004_pre_elections_Mbeki.txt', '2005_Mbeki.txt', '2006_Mbeki.txt', '2007_Mbeki.txt', '2008_Mbeki.txt', 
               '2009_post_elections_Zuma.txt', '2009_pre_elections_ Motlanthe.txt', '2010_Zuma.txt', '2011_Zuma.txt', '2012_Zuma.txt', '2013_Zuma.txt', 
               '2014_post_elections_Zuma.txt', '2014_pre_elections_Zuma.txt', '2015_Zuma.txt', '2016_Zuma.txt', '2017_Zuma.txt', '2018_Ramaphosa.txt', 
               '2019_post_elections_Ramaphosa.txt', '2019_pre_elections_Ramaphosa.txt', '2020_Ramaphosa.txt', '2021_Ramaphosa.txt', '2022_Ramaphosa.txt', '2023_Ramaphosa.txt')


this_speech = c()
this_speech[1] = readChar('Data/1994_post_elections_Mandela.txt', nchars = 27050)
this_speech[2] = readChar('Data/1994_pre_elections_deKlerk.txt', nchars = 12786)
this_speech[3] = readChar('Data/1995_Mandela.txt', nchars = 39019)
this_speech[4] = readChar('Data/1996_Mandela.txt', nchars = 39524)
this_speech[5] = readChar('Data/1997_Mandela.txt', nchars = 37489)
this_speech[6] = readChar('Data/1998_Mandela.txt', nchars = 45247)
this_speech[7] = readChar('Data/1999_post_elections_Mandela.txt', nchars = 34674)
this_speech[8] = readChar('Data/1999_pre_elections_Mandela.txt', nchars = 41225)
this_speech[9] = readChar('Data/2000_Mbeki.txt', nchars = 37552)
this_speech[10] = readChar('Data/2001_Mbeki.txt', nchars = 41719)
this_speech[11] = readChar('Data/2002_Mbeki.txt', nchars = 50544)
this_speech[12] = readChar('Data/2003_Mbeki.txt', nchars = 58284)
this_speech[13] = readChar('Data/2004_post_elections_Mbeki.txt', nchars = 34590)
this_speech[14] = readChar('Data/2004_pre_elections_Mbeki.txt', nchars = 39232)
this_speech[15] = readChar('Data/2005_Mbeki.txt', nchars = 54635)
this_speech[16] = readChar('Data/2006_Mbeki.txt', nchars = 48643)
this_speech[17] = readChar('Data/2007_Mbeki.txt', nchars = 48641)
this_speech[18] = readChar('Data/2008_Mbeki.txt', nchars = 44907)
this_speech[19] = readChar('Data/2009_post_elections_Zuma.txt', nchars = 31101)
this_speech[20] = readChar('Data/2009_pre_elections_ Motlanthe.txt', nchars = 47157)
this_speech[21] = readChar('Data/2010_Zuma.txt', nchars = 26384)
this_speech[22] = readChar('Data/2011_Zuma.txt', nchars = 33281)
this_speech[23] = readChar('Data/2012_Zuma.txt', nchars = 33376)
this_speech[24] = readChar('Data/2013_Zuma.txt', nchars = 36006)
this_speech[25] = readChar('Data/2014_post_elections_Zuma.txt', nchars = 29403)
this_speech[26] = readChar('Data/2014_pre_elections_Zuma.txt', nchars = 36233)
this_speech[27] = readChar('Data/2015_Zuma.txt', nchars = 32860)
this_speech[28] = readChar('Data/2016_Zuma.txt', nchars = 32464)
this_speech[29] = readChar('Data/2017_Zuma.txt', nchars = 35981)
this_speech[30] = readChar('Data/2018_Ramaphosa.txt', nchars = 33290)
this_speech[31] = readChar('Data/2019_post_elections_Ramaphosa.txt', nchars = 42112)
this_speech[32] = readChar('Data/2019_pre_elections_Ramaphosa.txt', nchars = 56960)
this_speech[33] = readChar('Data/2020_Ramaphosa.txt', nchars = 47910)
this_speech[34] = readChar('Data/2021_Ramaphosa.txt', nchars = 43352)
this_speech[35] = readChar('Data/2022_Ramaphosa.txt', nchars = 52972)
this_speech[36] = readChar('Data/2022_Ramaphosa.txt', nchars = 52972)


sona = data.frame(filename = filenames, 
                  speech = this_speech, 
                  stringsAsFactors = FALSE)

## extract year and president for each speech
sona$year = str_sub(sona$filename, start = 1, end = 4)
sona$president_13 = str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

## clean the sona dataset by adding the date and removing unnecessary text
replace_reg = '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

sona = sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start = 1, end = 30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', ''))

### Change speeches into sentences

speechList = list()
# speechList[[1]] = tokenize_sentences(sona$speech[1])

for (i in 1:36){
  
  speechList = append(speechList, tokenize_sentences(sona$speech[i]))
}

# speechList[[1]] = strsplit(sona$speech[1], "(?<=[^.][.][^.])", perl = TRUE)

save(sona, speechList, file = "SonaData.RData")

```

### Data Manipulation

The cleaned data set is then used to seperate each of the speeches into their sentences using tokenisation. The apostrophes and numbers are removed from each sentence in the data frame. A sentence identifier is added to the data frame. Columns are removed from the sentence data frame such that it only consists of the president name, the year of the speech, the speech sentences and the sentence identifier.

The sentence data frame is then used to create a word data frame that consists of the words found in each of the speech sentences. This is similarly done using tokenisation. Each word now has it's own row in the dataframe, along with its sentence identifier, the name of the president who said the sentence the word is in and the year of the speech. Stop words are removed from the dataframe to improve training model efficiency, and increase the separability between the different president's speeches.

## Bag of Words Models

Two forms of word frequencies are considered and compared in this paper. Count Frequency and Term Frequency--Inverse Document Frequency (TF-IDF). Count frequency is a measure of the number of occurences of the word in the data frame. TF-IDF use a combination of relative frequencies and document frequencies to assign a score to each word in the data frame. For these frequency data frames to be usable by the classification algorithms for predictive modelling, the data frames have to be structured such that each word has its own column with its frequencies. These frequency data frames are then joined with the sentence data frame to form the model data frames. These new data frames have a column for each word in the word data frame. Each row of the model data frame reflects a sentence said by one of the presidents. The president column highlights the surname of the president who said that sentence. The remaining columns represent the count frequency or TF-IDF of each word in that sentence. These data frames can then be used to train classification algorithms to predict the president who said the sentence. For the rest of this paper, when the bag of words data frame is referenced, it refers to the data frame with word columns that hold the count frequency of the words in the sentence. In contrast, the TF-IDF data frame refers to data set with word columns that contain the tf-id frequency of the word in the sentence.

```{r eval=FALSE, echo=FALSE}

### Load Cleaned Data

load("SonaData.RData")

### Models

# Separate speeches into sentences

speechSentences = as_tibble(sona) %>%
  rename(president = president_13) %>%
  unnest_tokens(sentences, speech, token = "sentences") %>%
  select(president, year, sentences) %>%
  mutate(sentences, sentences = str_replace_all(sentences, "â€™", "'")) %>%
  mutate(sentences, sentences = str_replace_all(sentences, "'", "")) %>%
  mutate(sentences, sentences = str_remove_all(sentences, "[0-9]")) %>%
  mutate(sentID = row_number())

wordsWithSentID = speechSentences %>% 
  unnest_tokens(word, sentences, token = 'regex', pattern = unnest_reg) %>%
  filter(str_detect(word, '[a-z]')) %>%
  filter(!word %in% stop_words$word) %>%
  select(sentID, president, year, word) 

#### Bag-Of-Words Model

bagWords = wordsWithSentID %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n) 

speechTDF = speechSentences %>%
  inner_join(wordsWithSentID) %>%
  group_by(sentID, word) %>%
  count() %>%  
  group_by(sentID) %>%
  mutate(total = sum(n)) %>%
  ungroup()

# left_join got confused just using president as a variable 
# due to there being other president variables in speechTDF
# change to presidentName

bagWords = speechTDF %>%
  select(sentID, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  left_join(speechSentences %>% 
              rename(presidentName = president) %>% 
              select(sentID, presidentName), by = "sentID") %>%
  select(sentID, presidentName, everything())

table(bagWords$presidentName)

# remove deKlerk and Motlanthe
# use 1500 sentences from each of the four presidents

sampledBOW = bagWords %>% 
  filter(presidentName == "Mandela" | presidentName == "Mbeki" | 
           presidentName == "Ramaphosa" | presidentName == "Zuma") %>%
  group_by(presidentName) %>% 
  slice_sample(n = 1500) %>% 
  ungroup()

sampledBOW

table(sampledBOW$presidentName)

set.seed(2023)

trainingIDSBOW = sampledBOW %>% 
  group_by(presidentName) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(sentID)

trainingSentencesBOW = sampledBOW %>%
  right_join(trainingIDSBOW, by = "sentID") %>%
  select(-sentID)

# if else repeat while function for in next break

trainingSentencesBOW = trainingSentencesBOW[, -c(495, 776, 1651)]

tAndVSentencesBOW = sampledBOW %>%
  anti_join(trainingIDSBOW, by = "sentID")

validationIDSBOW = tAndVSentencesBOW %>%
  group_by(presidentName) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(sentID)

validationSentencesBOW = tAndVSentencesBOW %>%
  right_join(validationIDSBOW, by = "sentID") %>%
  select(-sentID)

validationSentencesBOW = validationSentencesBOW[, -c(495, 776, 1651)]

testSentencesBOW = tAndVSentencesBOW %>%
  anti_join(validationIDSBOW, by = "sentID") %>%
  select(-sentID)

testSentencesBOW = testSentencesBOW[, -c(495, 776, 1651)]

#### TF-IDF Model

tf.idf = speechTDF %>%
  bind_tf_idf(word, sentID, n) %>% 
  select(sentID, word, tf_idf) %>%
  pivot_wider(names_from = word, 
              values_from = tf_idf, 
              values_fill = 0) %>%  
  left_join(speechSentences %>% 
              rename(presidentName = president) %>% 
              select(sentID, presidentName), by = "sentID")

# remove deKlerk and Motlanthe
# use 1500 sentences from each of the four presidents

sampled.TF.IDF = tf.idf %>% 
  filter(presidentName == "Mandela" | presidentName == "Mbeki" | 
           presidentName == "Ramaphosa" | presidentName == "Zuma") %>%
  group_by(presidentName) %>% 
  slice_sample(n = 1500) %>% 
  ungroup()

table(sampled.TF.IDF$presidentName)

set.seed(2023)

trainingIDS.TF.IDF = sampled.TF.IDF %>% 
  group_by(presidentName) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(sentID)

trainingSentences.TF.IDF = sampled.TF.IDF %>%
  right_join(trainingIDS.TF.IDF, by = "sentID") %>%
  select(-sentID)

# if else repeat while function for in next break
trainingSentences.TF.IDF = trainingSentences.TF.IDF[, -c(494, 775, 1650)]

tAndVSentences.TF.IDF = sampled.TF.IDF %>%
  anti_join(trainingIDS.TF.IDF, by = "sentID")

validationIDS.TF.IDF = tAndVSentences.TF.IDF %>%
  group_by(presidentName) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(sentID)

validationSentences.TF.IDF = tAndVSentences.TF.IDF %>%
  right_join(validationIDS.TF.IDF, by = "sentID") %>%
  select(-sentID)

validationSentences.TF.IDF = validationSentences.TF.IDF[, -c(494, 775, 1650)]

testSentences.TF.IDF = tAndVSentences.TF.IDF %>%
  anti_join(validationIDS.TF.IDF, by = "sentID") %>%
  select(-sentID)

testSentences.TF.IDF = testSentences.TF.IDF[, -c(494, 775, 1650)]

save(speechSentences, bagWords, speechTDF,
     sampledBOW, sampled.TF.IDF,
     trainingSentencesBOW, trainingSentences.TF.IDF,
     validationSentencesBOW, validationSentences.TF.IDF,
     testSentencesBOW, testSentences.TF.IDF,
     file = "SetSentenceData.RData")

save(trainingSentencesBOW, trainingSentences.TF.IDF,
     validationSentencesBOW, validationSentences.TF.IDF,
     testSentencesBOW, testSentences.TF.IDF,
     file = "dataForNN.RData")

```

## Classification Algorithms

### Classification Trees

Classification Trees are one of the more simple predictive models to implement and use. They utilize recursive partitioning to split the observations into their different classes based on the most influential variables. The end result is a tree-like structure with a variable on each "branch" that indicates a partition in the feature space. The tree can then be used to classify an observation it has not seen before based on where it resides in the feature space.

### Random Forests

Random Forests are an extension of decision trees, and thus classification trees. Where as Classification Trees classify an observation based off of a singular decision tree, random forests use an ensemble of classification trees to decide the class an observation belongs to. Each tree is formed using a subset of the variables in the data and a sample of the observations to ensure tree variation in the forest.

### Feed Forward Neural Network

Feed Forward Neural Networks are one of the more complex predictive models used and implemented in this study. The classification feed forward neural networks take in a a data frame of predictors, pass them through a hidden layer or multiple hidden layers of nodes (each with their own weight), and outputs the probability of belonging to each class. Activation functions are applied to both the hidden layer and the output layer. The activation function applied to the hidden layers allow for more complex relationship identification by the network. The activation function applied to output layer ensures that the predictions are in the same form as the response. For classification problems, the output layer has the soft-max activation function applied to it. The classification network uses a cross entropy loss function to measure the performance of the network. It then uses an backpropagation to find the weights that most minimise the loss function.

#### Keras Data Pre-Processing

To implement the Feed Forward Neural Network in R, using Keras, the data is required to undergo further pre-processing. The president class variable is transformed into an integer variable and one hot encoded. One hot encoding transforms the categorical single response column into multiple binary response columns, one for each president in the data frame. Additionally, the BOW data is scaled to make it compatible with Keras. 

## Training, Validation and Testing

Both the bag of words data frame and the TF-IDF data frame are split into training, validation and test data frames as follows:

-   70% of the observations from each president are sampled for the training data frame.
-   20% for the validation data frame. 
-   10% for the test data frame.

These data frames are created once, and used consistently through out the predictive modelling process to ensure evaluation consistency across classification algorithms. 

# 3. Results and Discussion.

```{r echo=FALSE}

### Load Cleaned Data

load("SonaData.RData")

```

## Exploratory Data Analysis

Before any predictive models are implemented, exploratory data analysis (EDA) is conducted on the speeches. The EDA highlights numerous interesting and substantial aspects of the speech data set. Below, @fig-noOfWords shows the number of words spoken by each president in each of the speeches. There are many fascinating features of each President's speeches that are identifiable. President Zuma uses fewer words in his SONA's in comparison to the other presidents. The address given by de Klerk, pre elections in 1994, is the shortest speech, by a substantial margin. President Mbeki and President Ramaphosa have the most words in their speeches and follow a similar sentiment in their first SONA after replacing the previous president. President Mbeki's first two speeches are comparable in length to President Mandela's speeches. Equivalently, President Ramaphosa's first address has a similar length to President Zuma's speeches. The consistency of speech length to previous president's is seemingly an important and notable factor for speech writers when producing the first address for a newly elected president.

```{r fig-noOfWords, fig.align='center', fig.height=6, fig.width=12, fig.cap="Number of Words used in each of the SONA's, colour coded by president.", echo=FALSE}

### Exploratory Data Analysis

words = tokenize_words(sona$speech)

## Plot the number of words against the year

edaPlotData = data.frame("Year" = sona$year,
                         "nWords" = sapply(words, length),
                         "President" = sona$president_13)

ggplot(edaPlotData, aes(x = Year, y = nWords, color = President, shape = President)) +
  geom_point(size = 10) +
  xlab("Year") + ylab("Number of Words") +
  scale_x_discrete(name = "Year", 
                   breaks = c("1994","1999","2004", "2009", 
                              "2014", "2019", "2023")) +
  scale_y_continuous(name = "Number of Words", 
                     breaks = c(3000, 6000, 9000)) +
  scale_shape_manual(values = rep(18, 6)) +
  theme_bw(base_size = 14)

```

@fig-sentLengthTime below shows the average length of a sentence in each of the SONAs over the years. They are colour coded by the president who delivered the speech. The figure further highlights the stand out sentiments from above while introducing new concepts of interest. President Zuma had the shortest speeches by word count and these speeches were on average comprised of short sentences. In contrast, President Mbeki tended to have his speeches comprised of longer sentences. It is interesting to note the immense contrast in average sentence length between President Mbeki's first term as president (pre 2004) and his second term as president (post 2004). In his second term, his average sentence length increases by five words. Finally, the juxtaposition of the positioning of President Ramaphosa's speeches in the plots below and above is intriguing. President Ramaphosa's speech length rivals President Mbeki. However, the average length of the sentences used in his SONAs are closer to President Zuma. This is a purposeful decision in the speech writing process. Further, speech analysis and research is necessary to ascertain the likely reason for this decision. 

```{r fig-sentLengthTime, fig.align='center', fig.height=6, fig.width=12, fig.cap="Average length of sentence in each of the SONA's, colour coded by president.", echo=FALSE}

## Plot the Average Sentence Length over time

avgSentenceLength = c()

for (j in 1:36){
  
  sentWords = sapply(tokenize_sentences(sona$speech[j]), tokenize_words)
  len = length(sentWords)
  sentL = c()
  
  for (k in 1:len){
    sentL[k] = sapply(sentWords[k], length)
  }
  
  avgSentenceLength[j] = mean(sentL)
}

edaPlotData1 = data.frame("Year" = sona$year,
                          "Length" = avgSentenceLength,
                          "President" = sona$president_13)

ggplot(edaPlotData1, aes(x = Year, y = Length, color = President, shape = President)) +
  geom_point(size = 10) +
  xlab("Year") + ylab("Average Sentence Length") +
  scale_x_discrete(name = "Year", 
                   breaks = c("1994","1999","2004", "2009", 
                              "2014", "2019", "2023")) +
  scale_y_continuous(breaks = c(20, 30, 40)) +
  scale_shape_manual(values = rep(18, 6)) +
  theme_bw(base_size = 14)

```


### Imbalanced Data

The most important feature highlighted in the exploratory data analysis above is that the data frame only has one SONA for both de Klerk and President Motlanthe. The reason why it is notable is that the sentence data frame is imbalanced. The multi-year presidents have over 1500 sentences in the data frame. In contrast, de Klerk and President Motlanthe both have under 300 sentences. If the entire data frame is used as is, these presidents would be disproportionately represented. The classification algorithms are likely to struggle to capture the class structure of sentences and more likely to develop a bias towards the presidents with the majority of the observations in the data frame. There are numerous methods of dealing with the imbalanced data. Oversampling algorithms would increase the representation of the two aforementioned presidents in the data set. However, the sentence count discrepancy between de Klerk and President Motlanthe and the other presidents suggest that the oversampling algorithms would need to create or sample vastly more sentence observations than observations in the data frame for these two presidents. @mohammed2020machine highlights that the increase in the risk of overfitting the data is a severe limitation of oversampling algorithms. Therefore, de Klerk and President Motlanthe's observations are removed from both the Bag of Words data frame and the TF-IDF data frame. Additionally, to deal with the imbalanced class structure of the data set among the remaining four presidents, 1500 sentence observations are randomly sampled from each president's observations in both data frames. These 6000 observations are used to create the training, validation and testing data frames used in the predictive modelling process of the classification algorithms.

## Classification Trees

```{r eval=FALSE, echo=FALSE}

### Load Different Datasets

load("SetSentenceData.RData")

## Classification Tree

tic()
bowFit = rpart(presidentName ~., 
               data = trainingSentencesBOW,
               method = "class")
toc()

trainFittedBOW = predict(bowFit, type = 'class')
trainPredBOW = table(trainingSentencesBOW$presidentName, trainFittedBOW)
classErrorBOW.CT = (1 - round(sum(diag(trainPredBOW))/sum(trainPredBOW), 3))

tic()
tf.idf.Fit = rpart(presidentName ~., 
                   data = trainingSentences.TF.IDF,
                   method = "class")
toc()

trainFitted.TF.IDF = predict(tf.idf.Fit, type = 'class')
trainPred.TF.IDF = table(trainingSentences.TF.IDF$presidentName, 
                         trainFitted.TF.IDF)
classError.TF.IDF.CT = (1 - round(sum(diag(trainPred.TF.IDF))/sum(trainPred.TF.IDF), 3))

```

A full classification tree is fit to both the bag of words training data frame and the TF-IDF training data frame. The misclassification rate for both of the two models on the training data is shown below in @tbl-ct. Both models perform poorly on their training data. The bag of words (BOW) model misclassified 72% of all the observations. The TF-IDF model misclassified 70% of the data frame's observations. 

|        | Training Misclassification % |
|:------:|:----------------------------:|
|   BOW  |              72              |
| TF-IDF |             70.2             |

: Classification Trees Training Misclassification Rate {#tbl-ct}

These models perform slightly better than a single president classification algorithm (attribute all sentences to one president) that would have a misclassification rate of 75%. @tbl-confCTBOW is a confusion matrix that highlights how the classification tree algorithm classifies observations in the BOW training data (columns) in comparison to the actual classification of the observations (rows). The table accentuates the largest issue the model (and the TF-IDF model) has. The majority of the sentences have been classified for President Ramaphosa. The model has then found multiple small splits in the feature space that has allowed it to identify a small subset of President Zuma and President Mbeki's observations. Additionally, the model does not classify any of the sentences for President Mandela. 


|           | Mandela | Mbeki | Ramaphosa | Zuma |
|:---------:|:-------:|:-----:|:---------:|:----:|
|  Mandela  |    0    |   31  |    1019   |   0  |
|   Mbeki   |    0    |   74  |    975    |   1  |
| Ramaphosa |    0    |   1   |    1049   |   0  |
|    Zuma   |    0    |   10  |    985    |  55  |

: Confusion Matrix for BOW Training Data {#tbl-confCTBOW}

## Random Forests

```{r eval=FALSE, echo=FALSE}

## Random Forests (using Ranger)

trainingSentencesBOW$presidentName = factor(trainingSentencesBOW$presidentName)
trainingSentences.TF.IDF$presidentName = factor(trainingSentences.TF.IDF$presidentName)

#### Validation Analysis with Ranger
#### Bag-Of-Words

errorBOW = c()
valErrBOW = c()
for(i in 1:length(seq)){
  tic()
  mod = ranger(presidentName ~ ., 
               data = trainingSentencesBOW,
               num.trees = seq[i], 
               mtry = 20)
  errorBOW[i] = mod$prediction.error
  valModPredsBOW = predict(mod, validationSentencesBOW)$predictions
  valModPredsBOW.Ranger = table(validationSentencesBOW$presidentName, 
                                valModPredsBOW)
  valErrBOW[i] = (1 - round(sum(diag(valModPredsBOW.Ranger))/sum(valModPredsBOW.Ranger), 3))
  print(paste0("Iteration: ", i))
  toc()
}

valPlotDataBOW = data.frame("Seq" = seq,
                            "Error" = errorBOW,
                            "ValError" = valErrBOW)

minTreesBOW = seq[which.min(valErrBOW)]

#### Validation Analysis with Ranger
#### tf-IDF

error = c()
valErr = c()
seq = seq(100, 3000, by = 100)
for(i in 1:length(seq)){
  tic()
  mod = ranger(presidentName ~ ., 
               data = trainingSentences.TF.IDF,
               num.trees = seq[i], 
               mtry = 20)
  error[i] = mod$prediction.error
  valModPreds = predict(mod, validationSentences.TF.IDF)$predictions
  valModPreds.Ranger = table(validationSentences.TF.IDF$presidentName, valModPreds)
  valErr[i] = (1 - round(sum(diag(valModPreds.Ranger))/sum(valModPreds.Ranger), 3))
  print(paste0("Iteration: ", i))
  toc()
}

valPlotData.TF.IDF = data.frame("Seq" = seq,
                                "Error" = error,
                                "ValError" = valErr)

minTrees.TF.IDF.RF = seq[which.min(valErr)]

```

### Validation Analysis

The classification random forest modelling process requires validation analysis to choose the optimal set of parameters that will minimise the misclassification rate for the final model on the data. In this study, the optimal number of trees in each random forest is crucial to the model's classification performance. @fig-BOWValRF shows the misclassification rate for random forest models on both the training and validation BOW data as the number of trees in the model increase from 100 trees to 3000 trees. The significant overlap between the rates for both the training data and the validation data suggests that the model is struggling to capture the class structure in the data and has not settled. The two main reasons for this issue are that the random forest model is ill-suited to deal with the data frame as is or a greater number of trees need to be considered in the BOW random forest model. Nonetheless, the random forest model with 2500 trees produces the lowest validation misclassification rate for the BOW data. Similarly, @fig-tfidfValRF shows the misclassification rate for random forest models on both the training and validation TF-IDF data for increasing numbers of trees. The difference in error between the training data and the validation data is more pronounced here, in contrast to the BOW random forest models. The TF-IDF random forest model with 1300 trees, misclassifies the least amount of sentences in the validation data. 

```{r fig-BOWValRF, fig.align='center', fig.height=6, fig.width=12, fig.cap="Misclassification Rate on training (black) and validation (red) BOW data for increasing number of trees in random forest. The blue dotted line indicates the random forest model that misclassified the smallest number of sentences in the validation data.", echo=FALSE}

### Load Data for Plots and Tables (saved from previous run of models)

load("T&RF.RData")

ggplot(valPlotDataBOW, aes(x = Seq)) +
  geom_line(aes(y = Error), linewidth = 3, linetype = 1, col = "black") +
  geom_line(aes(y = ValError), linewidth = 3, linetype = 1, col = "red") +
  geom_vline(xintercept = minTreesBOW, 
             linewidth = 1, linetype = 6, col = "blue") +
  xlab("Number of Trees") + ylab("Classification Error") +
  theme_bw(base_size = 14)

```



```{r fig-tfidfValRF, fig.align='center', fig.height=6, fig.width=12, fig.cap="Misclassification Rate on training (black) and validation (red) TF-IDF data for increasing number of trees in random forest. The blue dotted line indicates the random forest model that misclassified the smallest number of sentences in the validation data.", echo=FALSE}

ggplot(valPlotData.TF.IDF, aes(x = Seq)) +
  geom_line(aes(y = Error), linewidth = 3, linetype = 1, col = "black") +
  geom_line(aes(y = ValError), linewidth = 3, linetype = 1, col = "red") +
  geom_vline(xintercept = minTrees.TF.IDF.RF, 
             linewidth = 1, linetype = 6, col = "blue") +
  xlab("Number of Trees") + ylab("Classification Error") +
  theme_bw(base_size = 14)

```

### Models with Optimal Parameters

```{r eval=FALSE, echo=FALSE}

#### Fit RF with Best Parameters on Training and Val Sets together

tAndVSentencesBOW = as.data.frame(rbind(trainingSentencesBOW, 
                                        validationSentencesBOW))
tic()
bowFit.Ranger = ranger(presidentName ~ ., 
                       data = tAndVSentencesBOW,
                       mtry = 20,
                       num.trees = minTreesBOW)
toc()

trainFittedBOW.Ranger = bowFit.Ranger$predictions
trainPredBOW.Ranger = table(tAndVSentencesBOW$presidentName, 
                            trainFittedBOW.Ranger)
classErrorBOW.RF = round(bowFit.Ranger$prediction.error, 3)

tAndVSentences.TF.IDF = as.data.frame(rbind(trainingSentences.TF.IDF, 
                                            validationSentences.TF.IDF))
tic()
tf.idf.Fit.Ranger = ranger(presidentName ~ ., 
                           data = tAndVSentences.TF.IDF,
                           mtry = 20,
                           num.trees = minTrees.TF.IDF.RF)
toc()

trainFitted.TF.IDF.Ranger = tf.idf.Fit.Ranger$predictions
trainPred.TF.IDF.Ranger = table(tAndVSentences.TF.IDF$presidentName, trainFitted.TF.IDF.Ranger)
classError.TF.IDF.RF = round(tf.idf.Fit.Ranger$prediction.error, 3)

```


The optimal number of trees for the models of both data sets, found using validation analysis, are used to fit two random forest models on the combined training and validation data. 

The misclassification rate for both of the two models on the combined data sets is shown below in @tbl-rf. The random forest models improve on the misclassification rate, in contrast to the classification trees. These models misclassify under 50% of the sentences in their specified data sets. The bag of words (BOW) random forest model misclassified 49.7% of the observations. The TF-IDF random forest model misclassified 48.8% of the data frame's observations. 

|        | Model Misclassification % |
|:------:|:----------------------------:|
|   BOW  |             49.7             |
| TF-IDF |             48.8             |

: Random Forest Misclassification Rate {#tbl-rf}

@tbl-panel shows confusion matrices that highlight how the random forest classification algorithm classifies observations in the data (columns) in comparison to the actual classification of the observations (rows) for both data sets. @tbl-rfBOW shows the classification contrast of the optimal BOW random forest model. Similarly, @tbl-rfTFIDF highlights the difference in classification for the optimal TF-IDF random forest model. Alike to the issues that arose for the BOW Classification Tree model, the BOW random forest model is biased towards a president. Over 50% of sentences in the BOW data frame are classified as coming from a speech by President Zuma. In contrast, the TF-IDF random forest model does not suffer from the same problem. The classification from its model on its data is far more evenly spread, which suggests that it is far more successful in identifying the underlying class structure. However, the misclassification rate in @tbl-rf highlights that the performance of these these models are incredibly similar. 

::: {#tbl-panel layout-ncol=2}

|           | Mandela | Mbeki | Ramaphosa | Zuma |
|:---------:|:-------:|:-----:|:---------:|:----:|
|  Mandela  |   467   |  312  |     99    |  487 |
|   Mbeki   |   192   |  636  |    103    |  434 |
| Ramaphosa |    91   |  132  |    640    |  502 |
|    Zuma   |    69   |  138  |    154    | 1004 |


: Bag Of Words {#tbl-rfBOW}

|           | Mandela | Mbeki | Ramaphosa | Zuma |
|:---------:|:-------:|:-----:|:---------:|:----:|
|  Mandela  |   582   |  382  |     87    |  314 |
|   Mbeki   |   238   |  720  |    113    |  294 |
| Ramaphosa |   114   |  205  |    712    |  334 |
|    Zuma   |   150   |  220  |    215    |  780 |


: TF-IDF {#tbl-rfTFIDF}

Random Forest Confusion Matrices
:::

## Feed Forward Neural Network

```{r eval=FALSE, echo=FALSE}

### Load Different Datasets

load("dataForNN.RData")
set.seed(2023)

## Neural Networks

# Pre-processing the Target Variable

presTarget.train.BOW = as.integer(factor(trainingSentencesBOW$presidentName)) - 1
presTarget.train.TF.IDF = as.integer(factor(trainingSentences.TF.IDF$presidentName)) - 1

presTarget.val.BOW = as.integer(factor(validationSentencesBOW$presidentName)) - 1
presTarget.val.TF.IDF = as.integer(factor(validationSentences.TF.IDF$presidentName)) - 1

presTarget.test.BOW = as.integer(factor(testSentencesBOW$presidentName)) - 1
presTarget.test.TF.IDF = as.integer(factor(testSentences.TF.IDF$presidentName)) - 1

# Removing President Variable from Data sets and scale Bag-Of-Words Data

presData.train.BOW = trainingSentencesBOW %>% 
  select(-presidentName) %>% 
  scale()

presData.train.TF.IDF = trainingSentences.TF.IDF %>% 
  select(-presidentName) %>%
  as.matrix()

presData.val.BOW = validationSentencesBOW %>% 
  select(-presidentName) %>%
  scale(center = attr(presData.train.BOW, "scaled:center"), 
        scale = attr(presData.train.BOW, "scaled:scale"))

presData.val.TF.IDF = validationSentences.TF.IDF %>% 
  select(-presidentName) %>%
  as.matrix()

presData.test.BOW = testSentencesBOW %>% 
  select(-presidentName) %>%
  scale(center = attr(presData.train.BOW, "scaled:center"), 
        scale = attr(presData.train.BOW, "scaled:scale"))

presData.test.TF.IDF = testSentences.TF.IDF %>% 
  select(-presidentName) %>%
  as.matrix()

# Remove columns with NAs

presData.train.BOW.NEW = presData.train.BOW %>%
  replace(is.na(.), 0)

presData.val.BOW = presData.val.BOW %>%
  replace(is.na(.), 0)

presData.test.BOW = presData.test.BOW %>%
  replace(is.na(.), 0)
  
# One-Hot Encoding President Name variable

presTarget.train.BOW.OH = to_categorical(presTarget.train.BOW)
presTarget.train.TF.IDF.OH = to_categorical(presTarget.train.TF.IDF)

presTarget.val.BOW.OH = to_categorical(presTarget.val.BOW)
presTarget.val.TF.IDF.OH = to_categorical(presTarget.val.TF.IDF)

presTarget.test.BOW.OH = to_categorical(presTarget.test.BOW)
presTarget.test.TF.IDF.OH = to_categorical(presTarget.test.TF.IDF)

## Imp Variables

dimensions = dim(presData.train.BOW.NEW)
presNum = dim(presTarget.train.BOW.OH)[2]

```

### Hyperparameter Tuning

Hyperparameter Tuning is a crucial facet of the neural network modelling process. In comparison to the random forests, where the only hyperparameter optimised for the models was the number of trees, these feed forward neural networks require the optimisation of multiple parameters to find the model that best classifies the speech sentences to the president. The hyperparameters optimised for the purpose of the neural networks in this study are the hidden layer activation functions, the number of nodes on the hidden layer, the dropout rate, the batch size and the number of hidden layers.  

#### Activation Functions, Nodes, Dropout and Batch Size

Both the BOW and the TF-IDF training and validation data sets were used to choose the optimal number of hidden layer nodes, the optimal dropout rate and the optimal batch size for each of their respective feed forward neural network models. The optimal hidden layer activation function is found for the TF-IDF data and used for both of the models. The hyperparameters and the considered values are shown below in @tbl-hypTune. 

| Activation |  Nodes  | Dropout % | Batch Size |
|:----------:|:-------:|:---------:|:----------:|
|  ReLu      |   32    |    10     |     16     |
|  SeLu      |   64    |    25     |     32     |
|  TanH      |   128   |    40     |     64     |
|  Sigmoid   |   256   |    55     |    128     |
|     -      |   512   |    70     |    256     |
|     -      |   1024  |    85     |    512     |

: Considered Values for Hyperparameters {#tbl-hypTune}

The validation misclassification rate of TF-IDF feed forward neural networks for
all 864 combinations of these hyperparameters are found. @fig-tfidfNNTune shows the performance of a subset of these TF-IDF neural networks as the number of nodes on the hidden layer increase from 32 to 1024. The batch size and the dropout % are fixed at their optimally found values of 16 and 10%. The feed forward neural networks using the Rectified Linear Unit (ReLu) activation function on the single hidden layer are the only group of networks that improve in performance as the number of nodes on the hidden layer increase. All of the neural networks with the other activation functions perform better in terms of classification with a smaller number of nodes on the hidden layer. Additionally, the discrepancy in the minimum validation misclassification rate between the ReLu activation function and the other activation functions is noticeable. The optimal activation function to use on the hidden layer is thus the ReLu activation function. 1024 nodes is the optimal number for the hidden layer of the TF-IDF feed forward neural network model. 

```{r eval=FALSE, echo=FALSE}

## Model Tuning

##### Keras Function for Tuning Parameters

actFunc = c("relu", "selu", "tanh", "sigmoid")
hiddenLayer = 32 * (2^seq(0, 5))
dropOut = seq(0.1, 0.85, by = 0.15)
bSize = 16 * (2^seq(0, 5))

kerasHPTuning = function(actF, hidLayer, dpRate, b, X, Y, XVal, YVal){
  
  dim = dim(X)
  nn = keras_model_sequential()
  
  nn %>% 
    layer_dense(units = hidLayer, 
                activation = actF, 
                input_shape = dim[2]) %>%
    layer_dropout(rate = dpRate) %>%
    layer_dense(units = presNum, activation = "softmax")
  
  summary(nn)
  
  nn %>% compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(learning_rate = 0.01),
    metrics = c('accuracy'),
  )
  
  nn.History = nn %>% fit(
    X, Y, 
    epochs = 10, batch_size = b,
    validation_data = list(XVal, YVal),
    verbose = 1, shuffle = TRUE
  )
  
  return(nn.History$metrics$val_accuracy[10])
}

reluArray = array(0, dim = c(length(hiddenLayer), 
                             length(dropOut),
                             length(bSize)))

seluArray = array(0, dim = c(length(hiddenLayer), 
                             length(dropOut),
                             length(bSize)))

tanhArray = array(0, dim = c(length(hiddenLayer), 
                             length(dropOut),
                             length(bSize)))

sigmArray = array(0, dim = c(length(hiddenLayer), 
                             length(dropOut),
                             length(bSize)))

for (i in 1:length(hiddenLayer)){
  
  for (j in 1:length(dropOut)){
    
    for (k in 1:length(bSize)){
      
      reluArray[i, j, k] =  kerasHPTuning(actFunc[1],
                                          hiddenLayer[i],
                                          dropOut[j],
                                          bSize[k],
                                          X = presData.train.TF.IDF,
                                          Y = presTarget.train.TF.IDF.OH,
                                          XVal = presData.val.TF.IDF,
                                          YVal = presTarget.val.TF.IDF.OH)
      
      seluArray[i, j, k] = kerasHPTuning(actFunc[2],
                                         hiddenLayer[i],
                                         dropOut[j],
                                         bSize[k],
                                         X = presData.train.TF.IDF,
                                         Y = presTarget.train.TF.IDF.OH,
                                         XVal = presData.val.TF.IDF,
                                         YVal = presTarget.val.TF.IDF.OH)
      
      tanhArray[i, j, k] = kerasHPTuning(actFunc[3],
                                         hiddenLayer[i],
                                         dropOut[j],
                                         bSize[k],
                                         X = presData.train.TF.IDF,
                                         Y = presTarget.train.TF.IDF.OH,
                                         XVal = presData.val.TF.IDF,
                                         YVal = presTarget.val.TF.IDF.OH)
      
      sigmArray[i, j, k] = kerasHPTuning(actFunc[4],
                                         hiddenLayer[i],
                                         dropOut[j],
                                         bSize[k],
                                         X = presData.train.TF.IDF,
                                         Y = presTarget.train.TF.IDF.OH,
                                         XVal = presData.val.TF.IDF,
                                         YVal = presTarget.val.TF.IDF.OH)
    }
  }
}

maxValues = c(max(reluArray),
              max(seluArray),
              max(tanhArray),
              max(sigmArray))

which.max(reluArray)

bestActF = actFunc[which.max(maxValues)]
bestH = hiddenLayer[6]
bestDP = dropOut[1]
bestB = bSize[1]

## Bag-Of-Words Keras Tuning

reluBOWArray = array(0, dim = c(length(hiddenLayer), 
                                length(dropOut),
                                length(bSize)))

for (i in 1:length(hiddenLayer)){
  
  for (j in 1:length(dropOut)){
    
    for (k in 1:length(bSize)){
      
      reluBOWArray[i, j, k] =  kerasHPTuning(actFunc[1],
                                             hiddenLayer[i],
                                             dropOut[j],
                                             bSize[k],
                                             X = presData.train.BOW.NEW,
                                             Y = presTarget.train.BOW.OH,
                                             XVal = presData.val.BOW,
                                             YVal = presTarget.val.BOW.OH)
    }
  }
}

bestBOWActF = actFunc[1]
bestBOWH = hiddenLayer[3]
bestBOWDP = dropOut[4]
bestBOWB = bSize[6]

```

```{r fig-tfidfNNTune, fig.align='center', fig.height=6, fig.width=12, fig.cap="Validation Misclassification Rate for TF-IDF neural networks with various hidden layer activation functions for an increasing number of nodes on the hidden layer (batch size = 16 and dropout = 10%). The activation functions used are: Relu (black), Selu (red), Tanh (purple) and Sigmoid (green). The blue dotted line indicates the optimal number of nodes on a single hidden layer network.", echo=FALSE}

### Load Data for Plots and Tables (saved from previous run of models)

load("FeedForwardNN_Data.RData")

actFunc = c("relu", "selu", "tanh", "sigmoid")
hiddenLayer = 32 * (2^seq(0, 5))
dropOut = seq(0.1, 0.85, by = 0.15)
bSize = 16 * (2^seq(0, 5))

tuningPlotData.TF.IDF = data.frame("HL" = hiddenLayer,
                                   "MisClass_Relu" = 1-reluArray[, 1, 1],
                                   "MisClass_Selu" = 1-seluArray[, 1, 1],
                                   "MisClass_Tanh" = 1-tanhArray[, 1, 1],
                                   "MisClass_Sig" = 1-sigmArray[, 1, 1])

ggplot(tuningPlotData.TF.IDF, aes(x = HL)) +
  geom_line(aes(y = MisClass_Relu), linewidth = 3, linetype = 1, col = "black") +
  geom_line(aes(y = MisClass_Selu), linewidth = 3, linetype = 1, col = "red") +
  geom_line(aes(y = MisClass_Tanh), linewidth = 3, linetype = 1, col = "purple") +
  geom_line(aes(y = MisClass_Sig), linewidth = 3, linetype = 1, col = "green") +
  geom_vline(xintercept = 1024, 
             linewidth = 1, linetype = 6, col = "blue") +
  xlab("Number of Nodes on Hidden Layer") + ylab("Classification Error") +
  theme_bw(base_size = 14)

```

The ReLu activation function is the only activation considered in the tuning process of the hyperparameters for the BOW Feed Forward Neural Network. @fig-BOWNNTune shows the validation misclassification rate, as the number of nodes on the hidden layer increase, for a subset of the neural networks with the optimally chosen batch size fixed at 512 and the dropout rate fixed at the optimal 55%. The optimal number of nodes to have on the hidden layer, according to the BOW validation data, is 128. 

```{r fig-BOWNNTune, fig.align='center', fig.height=6, fig.width=12, fig.cap="Validation Misclassification Rate for BOW neural networks with the ReLu hidden layer activation function for an increasing number of nodes on the hidden layer (batch size = 512 and dropout = 55%). The blue dotted line indicates the optimal number of nodes on a single hidden layer network.", echo=FALSE}

tuningPlotData.BOW = data.frame("HL" = hiddenLayer[1:5],
                                "MisClass_Relu" = 1-reluBOWArray[1:5, 4, 6])

ggplot(tuningPlotData.BOW, aes(x = HL)) +
  geom_line(aes(y = MisClass_Relu), linewidth = 2, linetype = 1, col = "black") +
  geom_vline(xintercept = 128, 
             linewidth = 1, linetype = 6, col = "blue") +
  xlab("Number of Nodes on Hidden Layer") + ylab("Classification Error") +
  theme_bw(base_size = 14)

```


#### Number of Hidden Layers

```{r eval=FALSE, echo=FALSE}

##### Keras Function for Tuning Number of Layers

kerasLayerTuning = function(nLayer, X, Y, XVal, YVal){
  
  dim = dim(X)
  nn = keras_model_sequential()
  
  if (nLayer == 1){
    
    nn %>% 
      layer_dense(units = bestH, 
                  activation = bestActF, 
                  input_shape = dim[2]) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = presNum, activation = "softmax")
    
    summary(nn)
    
    nn %>% compile(
      loss = "categorical_crossentropy",
      optimizer = optimizer_rmsprop(learning_rate = 0.01),
      metrics = c('accuracy'),
    )
    
    nn.History = nn %>% fit(
      X, Y, 
      epochs = 10, batch_size = bestB,
      validation_data = list(XVal, YVal),
      verbose = 1, shuffle = TRUE
    )
  }
  
  if (nLayer == 2){
    
    nn %>% 
      layer_dense(units = bestH, 
                  activation = bestActF, 
                  input_shape = dim[2]) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/2, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = presNum, activation = "softmax")
    
    summary(nn)
    
    nn %>% compile(
      loss = "categorical_crossentropy",
      optimizer = optimizer_rmsprop(learning_rate = 0.01),
      metrics = c('accuracy'),
    )
    
    nn.History = nn %>% fit(
      X, Y, 
      epochs = 10, batch_size = bestB,
      validation_data = list(XVal, YVal),
      verbose = 1, shuffle = TRUE
    )
  }
  
  if (nLayer == 3){
    
    nn %>% 
      layer_dense(units = bestH, 
                  activation = bestActF, 
                  input_shape = dim[2]) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/2, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/4, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = presNum, activation = "softmax")
    
    summary(nn)
    
    nn %>% compile(
      loss = "categorical_crossentropy",
      optimizer = optimizer_rmsprop(learning_rate = 0.01),
      metrics = c('accuracy'),
    )
    
    nn.History = nn %>% fit(
      X, Y, 
      epochs = 10, batch_size = bestB,
      validation_data = list(XVal, YVal),
      verbose = 1, shuffle = TRUE
    )
  }
  
  if (nLayer == 4){
    
    nn %>% 
      layer_dense(units = bestH, 
                  activation = bestActF, 
                  input_shape = dim[2]) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/2, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/4, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = bestH/8, 
                  activation = bestActF) %>%
      layer_dropout(rate = bestDP) %>%
      layer_dense(units = presNum, activation = "softmax")
    
    summary(nn)
    
    nn %>% compile(
      loss = "categorical_crossentropy",
      optimizer = optimizer_rmsprop(learning_rate = 0.01),
      metrics = c('accuracy'),
    )
    
    nn.History = nn %>% fit(
      X, Y, 
      epochs = 10, batch_size = bestB,
      validation_data = list(XVal, YVal),
      verbose = 1, shuffle = TRUE
    )
  }
  
  return(mean(nn.History$metrics$val_accuracy[5:10]))
}

layers = 1:4
layError = c()
for (l in 1:length(layers)){
  
  layError[l] = kerasLayerTuning(layers[l],
                                 X = presData.train.TF.IDF,
                                 Y = presTarget.train.TF.IDF.OH,
                                 XVal = presData.val.TF.IDF,
                                 YVal = presTarget.val.TF.IDF.OH)
  }

```

The other hyperparameter considered for tuning in the study's neural network modelling process is the number of hidden layers. The optimal number of hidden layers is found using the TF-IDF training and validation data. The optimal hyperparameters found previously for the TF-IDF feed forward neural networks are used in these models. The only caveat is that the number of nodes on each additional hidden layer is half of the hidden layer before it. @tbl-hidLay shows the performance of the neural network model as the number of hidden layers in the model increase. The table suggests that additional hidden layers, decrease the model's identify the class structure within the data. 

|     Hidden Layers     |     1     |     2     |     3      |    4       |
|:---------------------:|:---------:|:---------:|:----------:|:----------:|
|  Misclassification %  |   42.9    |    43.9   |   44.9     |     44.7   |


: Validation Misclassification % as hidden layers are added {#tbl-hidLay}

### Models with Optimal Parameters

```{r eval=FALSE, echo=FALSE}

# Creating Feed-Forward Neural Network Model

## Bag-Of-Words

bowFit.FF.NN = keras_model_sequential()

bowFit.FF.NN %>% 
  layer_dense(units = bestBOWH, 
              activation = bestBOWActF, 
              input_shape = dimensions[2]) %>%
  layer_dropout(rate = bestBOWDP) %>%
  layer_dense(units = presNum, activation = "softmax")

summary(bowFit.FF.NN)

bowFit.FF.NN %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 0.01),
  metrics = c('accuracy'),
)

bowFit.FF.NN.History = bowFit.FF.NN %>% fit(
  presData.train.BOW.NEW, presTarget.train.BOW.OH, 
  epochs = 40, batch_size = bestBOWB,
  validation_data = list(presData.val.BOW, presTarget.val.BOW.OH),
  verbose = 1, shuffle = TRUE
)

plot(bowFit.FF.NN.History)

## TF-IDF

dimensions.tf.idf = dim(presData.train.TF.IDF)

tf.idf.Fit.FF.NN = keras_model_sequential()

tf.idf.Fit.FF.NN %>% 
  layer_dense(units = bestH, 
              activation = bestActF, 
              input_shape = dimensions.tf.idf[2]) %>%
  layer_dropout(rate = bestBOWDP) %>%
  layer_dense(units = presNum, activation = "softmax")

summary(tf.idf.Fit.FF.NN)

tf.idf.Fit.FF.NN %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 0.01),
  metrics = c('accuracy'),
)

tf.idf.Fit.FF.NN.History = tf.idf.Fit.FF.NN %>% fit(
  presData.train.TF.IDF, presTarget.train.TF.IDF.OH, 
  epochs = 20, batch_size = bestB,
  validation_data = list(presData.val.TF.IDF, presTarget.val.TF.IDF.OH),
  verbose = 1, shuffle = TRUE
)

plot(tf.idf.Fit.FF.NN.History)

```


The optimal hyperparameters used to train and fit the two feed forward neural networks, in addition to the validation misclassification rate of the models, are shown in @tbl-optHypNN. The drastic difference between the performance of the optimal feed forward neural network on the BOW and the TF-IDF data is sizable. The BOW neural network classifies close to 55% of the sentences incorrectly. In contrast, the TF-IDF neural network only misclassifies close to 40% of the sentences in the TF-IDF validation data frame. 

|           | Activation |  Nodes  | Dropout % | Batch Size | Hidden Layers | Misclass % |
|:---------:|:----------:|:-------:|:---------:|:----------:|:-------------:|:----------:|
|   BOW     |  ReLu      |   128   |    55     |     512    |       1       |    54.4    |
|   TF-IDF  |  ReLu      |   1024  |    10     |     16     |       1       |    42.3    |

: Optimal Hyperparameters and Model Performance {#tbl-optHypNN}

## Test Performance

```{r eval=FALSE, echo=FALSE}

bow.Test.CT.ClassError
bow.Test.RF.ClassError
bowFit.FF.NN.ClassError

tf.idf.Test.CT.ClassError
tf.idf.Test.RF.ClassError
tf.idf.Fit.FF.NN.ClassError
```

Each of the classification model's considered in this study have their predictive capabilities tested with the respective test data sets. The misclassification rates for the BOW models are calculated on the BOW test data. Similarly, the performance of the TF-IDF models is calculated on the TF-IDF test data. @tbl-testPerf shows the test misclassification rates for all six of the classification models, ranked overall from best to worst. The TF-IDF neural network is the best classifier produced in this study. It accurately misclassifies 40% of the sentences in the test data to the president who used them in a SONA. Conversely, the BOW Classification Tree is the worst performing model. It incorrectly classifies over 71% of test sentences. 

|                               |  Test Misclassification %    |
|:-----------------------------:|:----------------------------:|
|  Neural Network (TF-IDF)      |             40.0             |
|  Random Forest (TF-IDF)       |             47.4             |
|  Random Forest (BOW)          |             47.8             |
|  Neural Network (BOW)         |             54.4             |
|  Classificiation Tree (TF-IDF)|             70.2             |
|  Classificiation Tree (BOW)   |             71.7             |

: Performance of Models on Test Data {#tbl-testPerf}

The most interesting feature of the table is that the BOW random forest outperforms the BOW neural network in classifying sentences to presidents by over 6%. @tbl-panelTest shows the confusion matrices comparing the predicted president classification (columns) to the actual president classification (rows) for the test data for both models. @tbl-nnBOWTest highlights that the BOW neural network is seemingly classifying close to 50% of the test sentences as from President Mandela. This is similar to the issues experienced by both the BOW classification tree model and the BOW random forest model. Each of these BOW models exhibit a strong bias towards a president. Even though it is not as pronounced in @tbl-rfBOWTest, the BOW random forest model's bias towards classifying sentences for President Zuma is clear. These results suggest that all of the BOW models are struggling to identify the true class structure in the BOW sentence data. The consistency of the problem propose that the issue is with the construction of the BOW data frame with the count frequencies and not the models themselves. 

::: {#tbl-panelTest layout-ncol=2}

|           | Mandela | Mbeki | Ramaphosa | Zuma |
|:---------:|:-------:|:-----:|:---------:|:----:|
|  Mandela  |    52   |   29  |     8     |  46  |
|   Mbeki   |    18   |   69  |     9     |  39  |
| Ramaphosa |    11   |   18  |     68    |  38  |
|    Zuma   |    6    |   13  |     23    |  93  |


: Random Forest {#tbl-rfBOWTest}

|           | Mandela | Mbeki | Ramaphosa | Zuma |
|:---------:|:-------:|:-----:|:---------:|:----:|
|  Mandela  |   103   |   19  |     6     |   7  |
|   Mbeki   |    68   |   43  |     7     |  17  |
| Ramaphosa |    57   |   11  |     56    |  11  |
|    Zuma   |    59   |   14  |     18    |  44  |


: Neural Network {#tbl-nnBOWTest}

BOW Test Confusion Matrices
:::

# 4. Conclusion.

# References.

[
  {
    "objectID": "addition.html",
    "href": "addition.html",
    "title": "Adding numbers in R",
    "section": "",
    "text": "To add numbers in R use +\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "common.html",
    "href": "common.html",
    "title": "Most Commonly Used Words",
    "section": "",
    "text": "The top 20 words most frequently used by each of the four multi-year presidents in their SONA’s are shown below. As is expected, there are many commonly used words across all four presidents. “Government” and “South” are both in the Top 5 words for each of the presidents. Intuitively, this is sensible as the expectation of the SONA is for the president, as leader of the government, to discuss the government’s achievements over the past year and their hopes for the next year. Similarly, the word south is commonly used by each president for obvious reasons. A simple overview of these commonly used words would suggest that the most common theme across all of the speeches for each president is that of economic development and growth. As a developing country, it is logical for the president’s to assure the nation in these annual speeches of the development the economy has undergone and the hope for the growth it will undergo.\n\n\n\n\n\nFigure 1. President Mandela’s top 20 most frequently used words in his speeches. The top 5 are highlighted in pink.\n\n\n\n\n\n\n\n\n\nFigure 2. President Mbeki’s top 20 most frequently used words in his speeches. The top 5 are highlighted in cyan.\n\n\n\n\n\n\n\n\n\nFigure 3. President Zuma’s top 20 most frequently used words in his speeches. The top 5 are highlighted in green.\n\n\n\n\n\n\n\n\n\nFigure 4. President Ramaphosa’s top 20 most frequently used words in his speeches. The top 5 are highlighted in purple."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "",
    "text": "The SONA (State of the Nation Address) is a highlight of the South African political calendar. Every year, and after every election, the South African president addresses parliament and the nation with a speech that highlights what the government has achieved over the past year and outlines the government’s important objectives over the next year. Since the beginning of 1994, there have been 36 SONA’s, by 6 presidents, each with their own distinct tone and diction. The distinct nature of these speeches and their structure suggest that given a sentence from one of the speeches, the president who delivered it is identifiable to a human who has seen the speeches before. Could predictive classification models identify the president solely from a sentence from one of their speeches?\nAnalysis of the speeches given by presidents of other countries is abundant in the literature [see Miranda and Bringula (2021); Najarzadegan, Dabaghi, and Eslami-Rasekh (2017); Budiharto and Meiliana (2018)]. These analyses focus on sentiment analysis and topic modelling. The purpose of this paper is to compare the ability of three classification algorithms to predict the South African president given a sentence from one of their speeches. The three classification algorithms that are assessed in this paper are: Classification Trees, Random Forests and Feed Forward Neural Networks."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Data",
    "text": "Data\nEach of the 36 SONA’s undergo cleaning and manipulation to create datasets that are usable by the classification models in the prediction process.\n\nData Cleaning\nOne data frame is created using all 36 speeches. The dataframe consists of the speech identifier (in this instance, the name of the text file for that speech), and the speech itself. The name of the president who gave the speech and the year the speech was delivered are extracted from the speech identifier. The speech date is added to the dataframe by extracting it from the speech itself. Additionally, all unnecessary text is removed from each of the speeches.\n\n\nData Manipulation\nThe cleaned data set is then used to seperate each of the speeches into their sentences using tokenisation. The apostrophes and numbers are removed from each sentence in the data frame. A sentence identifier is added to the data frame. Columns are removed from the sentence data frame such that it only consists of the president name, the year of the speech, the speech sentences and the sentence identifier.\nThe sentence data frame is then used to create a word data frame that consists of the words found in each of the speech sentences. This is similarly done using tokenisation. Each word now has it’s own row in the dataframe, along with its sentence identifier, the name of the president who said the sentence the word is in and the year of the speech. Stop words are removed from the dataframe to improve training model efficiency, and increase the separability between the different president’s speeches."
  },
  {
    "objectID": "index.html#bag-of-words-models",
    "href": "index.html#bag-of-words-models",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Bag of Words Models",
    "text": "Bag of Words Models\nTwo forms of word frequencies are considered and compared in this paper. Count Frequency and Term Frequency–Inverse Document Frequency (TF-IDF). Count frequency is a measure of the number of occurences of the word in the data frame. TF-IDF use a combination of relative frequencies and document frequencies to assign a score to each word in the data frame. For these frequency data frames to be usable by the classification algorithms for predictive modelling, the data frames have to be structured such that each word has its own column with its frequencies. These frequency data frames are then joined with the sentence data frame to form the model data frames. These new data frames have a column for each word in the word data frame. Each row of the model data frame reflects a sentence said by one of the presidents. The president column highlights the surname of the president who said that sentence. The remaining columns represent the count frequency or TF-IDF of each word in that sentence. These data frames can then be used to train classification algorithms to predict the president who said the sentence. For the rest of this paper, when the bag of words data frame is referenced, it refers to the data frame with word columns that hold the count frequency of the words in the sentence. In contrast, the TF-IDF data frame refers to data set with word columns that contain the tf-id frequency of the word in the sentence."
  },
  {
    "objectID": "index.html#classification-algorithms",
    "href": "index.html#classification-algorithms",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nClassification Trees\nClassification Trees are one of the more simple predictive models to implement and use. They utilize recursive partitioning to split the observations into their different classes based on the most influential variables. The end result is a tree-like structure with a variable on each “branch” that indicates a partition in the feature space. The tree can then be used to classify an observation it has not seen before based on where it resides in the feature space.\n\n\nRandom Forests\nRandom Forests are an extension of decision trees, and thus classification trees. Where as Classification Trees classify an observation based off of a singular decision tree, random forests use an ensemble of classification trees to decide the class an observation belongs to. Each tree is formed using a subset of the variables in the data and a sample of the observations to ensure tree variation in the forest.\n\n\nFeed Forward Neural Network\nFeed Forward Neural Networks are one of the more complex predictive models used and implemented in this study. The classification feed forward neural networks take in a a data frame of predictors, pass them through a hidden layer or multiple hidden layers of nodes (each with their own weight), and outputs the probability of belonging to each class. Activation functions are applied to both the hidden layer and the output layer. The activation function applied to the hidden layers allow for more complex relationship identification by the network. The activation function applied to output layer ensures that the predictions are in the same form as the response. For classification problems, the output layer has the soft-max activation function applied to it. The classification network uses a cross entropy loss function to measure the performance of the network. It then uses an backpropagation to find the weights that most minimise the loss function.\n\nKeras Data Pre-Processing\nTo implement the Feed Forward Neural Network in R, using Keras, the data is required to undergo further pre-processing. The president class variable is transformed into an integer variable and one hot encoded. One hot encoding transforms the categorical single response column into multiple binary response columns, one for each president in the data frame. Additionally, the BOW data is scaled to make it compatible with Keras."
  },
  {
    "objectID": "index.html#training-validation-and-testing",
    "href": "index.html#training-validation-and-testing",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Training, Validation and Testing",
    "text": "Training, Validation and Testing\nBoth the bag of words data frame and the TF-IDF data frame are split into training, validation and test data frames as follows:\n\n70% of the observations from each president are sampled for the training data frame.\n20% for the validation data frame.\n10% for the test data frame.\n\nThese data frames are created once, and used consistently through out the predictive modelling process to ensure evaluation consistency across classification algorithms."
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore any predictive models are implemented, exploratory data analysis (EDA) is conducted on the speeches. The EDA highlights numerous interesting and substantial aspects of the speech data set. Below, Figure 1 shows the number of words spoken by each president in each of the speeches. There are many fascinating features of each President’s speeches that are identifiable. President Zuma uses fewer words in his SONA’s in comparison to the other presidents. The address given by de Klerk, pre elections in 1994, is the shortest speech, by a substantial margin. President Mbeki and President Ramaphosa have the most words in their speeches and follow a similar sentiment in their first SONA after replacing the previous president. President Mbeki’s first two speeches are comparable in length to President Mandela’s speeches. Equivalently, President Ramaphosa’s first address has a similar length to President Zuma’s speeches. The consistency of speech length to previous president’s is seemingly an important and notable factor for speech writers when producing the first address for a newly elected president.\n\n\n\n\n\nFigure 1. Number of Words used in each of the SONA’s, colour coded by president.\n\n\n\n\nFigure 2 below shows the average length of a sentence in each of the SONAs over the years. They are colour coded by the president who delivered the speech. The figure further highlights the stand out sentiments from above while introducing new concepts of interest. President Zuma had the shortest speeches by word count and these speeches were on average comprised of short sentences. In contrast, President Mbeki tended to have his speeches comprised of longer sentences. It is interesting to note the immense contrast in average sentence length between President Mbeki’s first term as president (pre 2004) and his second term as president (post 2004). In his second term, his average sentence length increases by five words. Finally, the juxtaposition of the positioning of President Ramaphosa’s speeches in the plots below and above is intriguing. President Ramaphosa’s speech length rivals President Mbeki. However, the average length of the sentences used in his SONAs are closer to President Zuma. This is a purposeful decision in the speech writing process. Further, speech analysis and research is necessary to ascertain the likely reason for this decision.\n\n\n\n\n\nFigure 2. Average length of sentence in each of the SONA’s, colour coded by president.\n\n\n\n\n\nImbalanced Data\nThe most important feature highlighted in the exploratory data analysis above is that the data frame only has one SONA for both de Klerk and President Motlanthe. The reason why it is notable is that the sentence data frame is imbalanced. The multi-year presidents have over 1500 sentences in the data frame. In contrast, de Klerk and President Motlanthe both have under 300 sentences. If the entire data frame is used as is, these presidents would be disproportionately represented. The classification algorithms are likely to struggle to capture the class structure of sentences and more likely to develop a bias towards the presidents with the majority of the observations in the data frame. There are numerous methods of dealing with the imbalanced data. Oversampling algorithms would increase the representation of the two aforementioned presidents in the data set. However, the sentence count discrepancy between de Klerk and President Motlanthe and the other presidents suggest that the oversampling algorithms would need to create or sample vastly more sentence observations than observations in the data frame for these two presidents. Mohammed, Rawashdeh, and Abdullah (2020) highlights that the increase in the risk of overfitting the data is a severe limitation of oversampling algorithms. Therefore, de Klerk and President Motlanthe’s observations are removed from both the Bag of Words data frame and the TF-IDF data frame. Additionally, to deal with the imbalanced class structure of the data set among the remaining four presidents, 1500 sentence observations are randomly sampled from each president’s observations in both data frames. These 6000 observations are used to create the training, validation and testing data frames used in the predictive modelling process of the classification algorithms."
  },
  {
    "objectID": "index.html#classification-trees-1",
    "href": "index.html#classification-trees-1",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Classification Trees",
    "text": "Classification Trees\nA full classification tree is fit to both the bag of words training data frame and the TF-IDF training data frame. The misclassification rate for both of the two models on the training data is shown below in Table 1. Both models perform poorly on their training data. The bag of words (BOW) model misclassified 72% of all the observations. The TF-IDF model misclassified 70% of the data frame’s observations.\n\n\nTable 1. Classification Trees Training Misclassification Rate\n\n\n\nTraining Misclassification %\n\n\n\n\nBOW\n72\n\n\nTF-IDF\n70.2\n\n\n\n\nThese models perform slightly better than a single president classification algorithm (attribute all sentences to one president) that would have a misclassification rate of 75%. Table 2 is a confusion matrix that highlights how the classification tree algorithm classifies observations in the BOW training data (columns) in comparison to the actual classification of the observations (rows). The table accentuates the largest issue the model (and the TF-IDF model) has. The majority of the sentences have been classified for President Ramaphosa. The model has then found multiple small splits in the feature space that has allowed it to identify a small subset of President Zuma and President Mbeki’s observations. Additionally, the model does not classify any of the sentences for President Mandela.\n\n\nTable 2. Confusion Matrix for BOW Training Data\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nMandela\n0\n31\n1019\n0\n\n\nMbeki\n0\n74\n975\n1\n\n\nRamaphosa\n0\n1\n1049\n0\n\n\nZuma\n0\n10\n985\n55"
  },
  {
    "objectID": "index.html#random-forests-1",
    "href": "index.html#random-forests-1",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Random Forests",
    "text": "Random Forests\n\nValidation Analysis\nThe classification random forest modelling process requires validation analysis to choose the optimal set of parameters that will minimise the misclassification rate for the final model on the data. In this study, the optimal number of trees in each random forest is crucial to the model’s classification performance. Figure 3 shows the misclassification rate for random forest models on both the training and validation BOW data as the number of trees in the model increase from 100 trees to 3000 trees. The significant overlap between the rates for both the training data and the validation data suggests that the model is struggling to capture the class structure in the data and has not settled. The two main reasons for this issue are that the random forest model is ill-suited to deal with the data frame as is or a greater number of trees need to be considered in the BOW random forest model. Nonetheless, the random forest model with 2500 trees produces the lowest validation misclassification rate for the BOW data. Similarly, Figure 4 shows the misclassification rate for random forest models on both the training and validation TF-IDF data for increasing numbers of trees. The difference in error between the training data and the validation data is more pronounced here, in contrast to the BOW random forest models. The TF-IDF random forest model with 1300 trees, misclassifies the least amount of sentences in the validation data.\n\n\n\n\n\nFigure 3. Misclassification Rate on training (black) and validation (red) BOW data for increasing number of trees in random forest. The blue dotted line indicates the random forest model that misclassified the smallest number of sentences in the validation data.\n\n\n\n\n\n\n\n\n\nFigure 4. Misclassification Rate on training (black) and validation (red) TF-IDF data for increasing number of trees in random forest. The blue dotted line indicates the random forest model that misclassified the smallest number of sentences in the validation data.\n\n\n\n\n\n\nModels with Optimal Parameters\nThe optimal number of trees for the models of both data sets, found using validation analysis, are used to fit two random forest models on the combined training and validation data.\nThe misclassification rate for both of the two models on the combined data sets is shown below in Table 3. The random forest models improve on the misclassification rate, in contrast to the classification trees. These models misclassify under 50% of the sentences in their specified data sets. The bag of words (BOW) random forest model misclassified 49.7% of the observations. The TF-IDF random forest model misclassified 48.8% of the data frame’s observations.\n\n\nTable 3. Random Forest Misclassification Rate\n\n\n\nModel Misclassification %\n\n\n\n\nBOW\n49.7\n\n\nTF-IDF\n48.8\n\n\n\n\nTable 4 shows confusion matrices that highlight how the random forest classification algorithm classifies observations in the data (columns) in comparison to the actual classification of the observations (rows) for both data sets. Table 4 (a) shows the classification contrast of the optimal BOW random forest model. Similarly, Table 4 (b) highlights the difference in classification for the optimal TF-IDF random forest model. Alike to the issues that arose for the BOW Classification Tree model, the BOW random forest model is biased towards a president. Over 50% of sentences in the BOW data frame are classified as coming from a speech by President Zuma. In contrast, the TF-IDF random forest model does not suffer from the same problem. The classification from its model on its data is far more evenly spread, which suggests that it is far more successful in identifying the underlying class structure. However, the misclassification rate in Table 3 highlights that the performance of these these models are incredibly similar.\n\n\nTable 4. Random Forest Confusion Matrices\n\n\n\n\n(a) Bag Of Words\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nMandela\n467\n312\n99\n487\n\n\nMbeki\n192\n636\n103\n434\n\n\nRamaphosa\n91\n132\n640\n502\n\n\nZuma\n69\n138\n154\n1004\n\n\n\n\n\n\n(b) TF-IDF\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nMandela\n582\n382\n87\n314\n\n\nMbeki\n238\n720\n113\n294\n\n\nRamaphosa\n114\n205\n712\n334\n\n\nZuma\n150\n220\n215\n780"
  },
  {
    "objectID": "index.html#feed-forward-neural-network-1",
    "href": "index.html#feed-forward-neural-network-1",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Feed Forward Neural Network",
    "text": "Feed Forward Neural Network\n\nHyperparameter Tuning\nHyperparameter Tuning is a crucial facet of the neural network modelling process. In comparison to the random forests, where the only hyperparameter optimised for the models was the number of trees, these feed forward neural networks require the optimisation of multiple parameters to find the model that best classifies the speech sentences to the president. The hyperparameters optimised for the purpose of the neural networks in this study are the hidden layer activation functions, the number of nodes on the hidden layer, the dropout rate, the batch size and the number of hidden layers.\n\nActivation Functions, Nodes, Dropout and Batch Size\nBoth the BOW and the TF-IDF training and validation data sets were used to choose the optimal number of hidden layer nodes, the optimal dropout rate and the optimal batch size for each of their respective feed forward neural network models. The optimal hidden layer activation function is found for the TF-IDF data and used for both of the models. The hyperparameters and the considered values are shown below in Table 5.\n\n\nTable 5. Considered Values for Hyperparameters\n\n\nActivation\nNodes\nDropout %\nBatch Size\n\n\n\n\nReLu\n32\n10\n16\n\n\nSeLu\n64\n25\n32\n\n\nTanH\n128\n40\n64\n\n\nSigmoid\n256\n55\n128\n\n\n-\n512\n70\n256\n\n\n-\n1024\n85\n512\n\n\n\n\nThe validation misclassification rate of TF-IDF feed forward neural networks for all 864 combinations of these hyperparameters are found. Figure 5 shows the performance of a subset of these TF-IDF neural networks as the number of nodes on the hidden layer increase from 32 to 1024. The batch size and the dropout % are fixed at their optimally found values of 16 and 10%. The feed forward neural networks using the Rectified Linear Unit (ReLu) activation function on the single hidden layer are the only group of networks that improve in performance as the number of nodes on the hidden layer increase. All of the neural networks with the other activation functions perform better in terms of classification with a smaller number of nodes on the hidden layer. Additionally, the discrepancy in the minimum validation misclassification rate between the ReLu activation function and the other activation functions is noticeable. The optimal activation function to use on the hidden layer is thus the ReLu activation function. 1024 nodes is the optimal number for the hidden layer of the TF-IDF feed forward neural network model.\n\n\n\n\n\nFigure 5. Validation Misclassification Rate for TF-IDF neural networks with various hidden layer activation functions for an increasing number of nodes on the hidden layer (batch size = 16 and dropout = 10%). The activation functions used are: Relu (black), Selu (red), Tanh (purple) and Sigmoid (green). The blue dotted line indicates the optimal number of nodes on a single hidden layer network.\n\n\n\n\nThe ReLu activation function is the only activation considered in the tuning process of the hyperparameters for the BOW Feed Forward Neural Network. Figure 6 shows the validation misclassification rate, as the number of nodes on the hidden layer increase, for a subset of the neural networks with the optimally chosen batch size fixed at 512 and the dropout rate fixed at the optimal 55%. The optimal number of nodes to have on the hidden layer, according to the BOW validation data, is 128.\n\n\n\n\n\nFigure 6. Validation Misclassification Rate for BOW neural networks with the ReLu hidden layer activation function for an increasing number of nodes on the hidden layer (batch size = 512 and dropout = 55%). The blue dotted line indicates the optimal number of nodes on a single hidden layer network.\n\n\n\n\n\n\nNumber of Hidden Layers\nThe other hyperparameter considered for tuning in the study’s neural network modelling process is the number of hidden layers. The optimal number of hidden layers is found using the TF-IDF training and validation data. The optimal hyperparameters found previously for the TF-IDF feed forward neural networks are used in these models. The only caveat is that the number of nodes on each additional hidden layer is half of the hidden layer before it. Table 6 shows the performance of the neural network model as the number of hidden layers in the model increase. The table suggests that additional hidden layers, decrease the model’s identify the class structure within the data.\n\n\nTable 6. Validation Misclassification % as hidden layers are added\n\n\nHidden Layers\n1\n2\n3\n4\n\n\n\n\nMisclassification %\n42.9\n43.9\n44.9\n44.7\n\n\n\n\n\n\n\nModels with Optimal Parameters\nThe optimal hyperparameters used to train and fit the two feed forward neural networks, in addition to the validation misclassification rate of the models, are shown in Table 7. The drastic difference between the performance of the optimal feed forward neural network on the BOW and the TF-IDF data is sizable. The BOW neural network classifies close to 55% of the sentences incorrectly. In contrast, the TF-IDF neural network only misclassifies close to 40% of the sentences in the TF-IDF validation data frame.\n\n\nTable 7. Optimal Hyperparameters and Model Performance\n\n\n\n\n\n\n\n\n\n\n\n\nActivation\nNodes\nDropout %\nBatch Size\nHidden Layers\nMisclass %\n\n\n\n\nBOW\nReLu\n128\n55\n512\n1\n54.4\n\n\nTF-IDF\nReLu\n1024\n10\n16\n1\n42.3"
  },
  {
    "objectID": "index.html#test-performance",
    "href": "index.html#test-performance",
    "title": "Predicting the President: An Exploration and Comparison of Classification algorithms.",
    "section": "Test Performance",
    "text": "Test Performance\nEach of the classification model’s considered in this study have their predictive capabilities tested with the respective test data sets. The misclassification rates for the BOW models are calculated on the BOW test data. Similarly, the performance of the TF-IDF models is calculated on the TF-IDF test data. Table 8 shows the test misclassification rates for all six of the classification models, ranked overall from best to worst. The TF-IDF neural network is the best classifier produced in this study. It accurately misclassifies 40% of the sentences in the test data to the president who used them in a SONA. Conversely, the BOW Classification Tree is the worst performing model. It incorrectly classifies over 71% of test sentences.\n\n\nTable 8. Performance of Models on Test Data\n\n\n\nTest Misclassification %\n\n\n\n\nNeural Network (TF-IDF)\n40.0\n\n\nRandom Forest (TF-IDF)\n47.4\n\n\nRandom Forest (BOW)\n47.8\n\n\nNeural Network (BOW)\n54.4\n\n\nClassificiation Tree (TF-IDF)\n70.2\n\n\nClassificiation Tree (BOW)\n71.7\n\n\n\n\nThe most interesting feature of the table is that the BOW random forest outperforms the BOW neural network in classifying sentences to presidents by over 6%. Table 9 shows the confusion matrices comparing the predicted president classification (columns) to the actual president classification (rows) for the test data for both models. Table 9 (b) highlights that the BOW neural network is seemingly classifying close to 50% of the test sentences as from President Mandela. This is similar to the issues experienced by both the BOW classification tree model and the BOW random forest model. Each of these BOW models exhibit a strong bias towards a president. Even though it is not as pronounced in Table 9 (a), the BOW random forest model’s bias towards classifying sentences for President Zuma is clear. These results suggest that all of the BOW models are struggling to identify the true class structure in the BOW sentence data. The consistency of the problem indicates that the issue is with the construction of the BOW data frame with the count frequencies and not the models themselves.\n\n\nTable 9. BOW Test Confusion Matrices\n\n\n\n\n(a) Random Forest\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nMandela\n52\n29\n8\n46\n\n\nMbeki\n18\n69\n9\n39\n\n\nRamaphosa\n11\n18\n68\n38\n\n\nZuma\n6\n13\n23\n93\n\n\n\n\n\n\n(b) Neural Network\n\n\n\nMandela\nMbeki\nRamaphosa\nZuma\n\n\n\n\nMandela\n103\n19\n6\n7\n\n\nMbeki\n68\n43\n7\n17\n\n\nRamaphosa\n57\n11\n56\n11\n\n\nZuma\n59\n14\n18\n44"
  },
  {
    "objectID": "multiplication.html",
    "href": "multiplication.html",
    "title": "Multiplying numbers in R",
    "section": "",
    "text": "To multiply numbers in R use *\n\n2 * 3\n\n[1] 6"
  }
]